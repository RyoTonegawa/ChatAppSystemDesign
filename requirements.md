WebSocketやConsistentHashingを理解するために双方向通信のチャットアプリを実装したい。

技術スタック
- フロント　NextJS　Shadcn
- バック　NestJS　Prisma Swagger オニオンアーキテクチャs
- メッセージブローカー　Kafka
- DB PostgreSQL Row-level Security

要件


機能要件
* ユーザーはブラウザまたはモバイルアプリから双方向チャットを行える
* 1対1チャットと最大50人までの小規模グループチャットを提供する
* チャットは退出と参加が好きにできる
* メッセージはテキストのみ、1件あたり最大1000文字
* ユーザーはオンラインプレセンスの状態が分かる


非機能要件
* 高可用性、高信頼性、メッセージは消失しない
* 低レイテンシ、往復遅延は世界中でおおよそ250ms以下を目指す
* 水平スケーラビリティ、同時接続1億を目指す


想定質問
❓️オンラインプレセンスの条件は？
→ アプリを閉じるなど30秒以上接続が途切れていたら、オフライン扱い
❓️ユーザーの読み取りと書き込みの比率は？
→ おおよそ1:1


データモデル
Users
* id: ユニークID
* name: ユーザー名
💡
ユーザー属性は最低限にします。
Channels
* id: ユニークID
* type: directまたはgroup
* createdAt
💡
directチャネルは1対1のチャット、groupチャネルは多人数参加のチャット
Memberships
* userId
* channelId
* joinedAt
💡
directチャネルは1対1のチャット、groupチャネルは多人数参加のチャット
Messages
* id
* channelId
* senderId
* body
* createdAt
* seq: チャネル内で単調増加するシーケンス番号
* status: draft、sent、failed
UserPresences
* userId
* status: online、offline
* lastEventAt: 最後にステータスが変わった時間


API
多くの他のプロダクトとは異なり、REST APIが適している場合が多いですが、リアルタイムチャットでは高頻度で遅延なく通信がする事が求められます。なので今回はWebSocketを使用します。ユーザーがアプリを開くとサーバーに接続し、このWebSocketの接続を確立して、APIとしてコマンドの送受信に利用します。詳しい考察は「設計の深堀りについて」で行います。


チャネルを作成する
クライアントはWebSocket経由でcreate_channelコマンドを送信します。1対1チャネルの場合はメンバー2人、グループチャネルの場合は最大50人まで指定可能です。
リクエスト

{
  "type": "create_channel",
  "payload": {
    "channelType": "direct" | "group",
    "memberIds": ["userA", "userB", "..."]
  }
レスポンス

{
  "type": "channel_created",
  "payload": {
    "channelId": "uuid-abc",
    "createdAt": "2025-06-01T00:00:00Z"
  }
}


メッセージを送信する
クライアントはsend_messageコマンドでメッセージを送信します。サーバーは永続化後にmessage_sentイベントを返し、他のメンバーにもブロードキャストします。
リクエスト

{
  "type": "send_message",
  "payload": {
    "channelId": "uuid-abc",
    "body": "こんにちは",
    "clientMessageId": "uuid-1234"
  }
}
レスポンス

{
  "type": "message_sent",
  "payload": {
    "channelId": "uuid-abc",
    "messageId": "uuid-1234",
    "senderId": "userA",
    "body": "こんにちは",
    "createdAt": "2025-06-01T00:00:01Z",
    "seq": 101
  }
}
備考
* clientMessageIdは重複送信対策（冪等性）に使う
* 書き込み成功後にのみシーケンス番号を割り当てる


メッセージを受信する
他のユーザーが送信したメッセージは、該当チャネルの参加者全員にmessage_receivedイベントとして配信されます。
受信イベント

{
  "type": "message_received",
  "payload": {
    "channelId": "uuid-abc",
    "messageId": "uuid-1234",
    "senderId": "userA",
    "body": "こんにちは",
    "createdAt": "2025-06-01T00:00:01Z",
    "seq": 101
  }
}
備考
* メッセージは順序保証のあるブロードキャスト（seq順）で配信


チャネルに参加・離脱する
WebSocket接続中のクライアントは、join_channelやleave_channelコマンドで動的にチャネル参加を制御できます。
参加

{
  "type": "join_channel",
  "payload": {
    "channelId": "uuid-abc"
  }
}
離脱

{
  "type": "leave_channel",
  "payload": {
    "channelId": "uuid-abc"
  }
}
レスポンス

{
  "type": "channel_joined" | "channel_left",
  "payload": {
    "channelId": "uuid-abc"
  }
}
備考
* join_channelはリアルタイム購読を開始する意味で、サーバーが以後そのチャネルのイベントをプッシュするようになる


オンラインプレセンス（heartbeat）を送信する
クライアントは定期的にheartbeatコマンドを送信して、自身がオンラインもしくはオフラインであることをサーバーに通知します。サーバーはその情報を使って、他ユーザーにプレセンスの更新を通知したり、アイドル状態と判定したりします。
リクエスト

{
  "type": "heartbeat",
  "payload": {
    "userId": "userA",
    "status": "online"
  }
}


オンラインプレセンス通知を受信する
他のユーザーがオンラインになった（または切断された）場合、同じチャネル内のメンバーにはpresence_updatedイベントとして通知されます。
受信イベント

{
  "type": "presence_updated",
  "payload": {
    "userId": "userA",
    "status": "online" | "offline",
    "lastEventAt": "2025-06-01T00:00:10Z"
  }
}


アーキテクチャ図
* Chat Serviceはチャネルの作成・参加・離脱、メッセージの送受信の役割を担う
* Presence Serviceはユーザーのプレセンスをオンライン、オフラインを管理します
* User Serviceはユーザーの情報を取得する
* 高負荷な書き込みが予想されるデータストアはKVSを用いて、その他はRDB
￼


設計の深堀りについて


1. リアルタイム通信方式の比較
非機能要件には低レイテンシでメッセージを送受信する仕組みが必要とされています。ここでは、リアルタイム通信を実現する、HTTPショートポーリング・ロングポーリング、そして双方向通信のWebSocketの三つを考察し、今回のユースケースにあった手法を選択します。
関連
* 📗 サーバー側の更新をリアルタイムに受け取る（Polling / Long Polling / SSE）  
* 📗 リアルタイム双方向通信（WebSocket）  

ショートポーリング
クライアントが固定間隔でHTTPリクエストを送信し、新着メッセージの有無を確認します。実装が非常に簡単です。
欠点
* メッセージが届いていなくてもリクエストが発生するため、同時接続が増えると無駄なリクエストが大量に発生する事になります。
* チャットのように低レイテンシが必須のユースケースではポーリング間隔を短く設定しがちで、結果的に「高負荷なのにリアルタイムではない」という矛盾を抱えやすいです。
￼

ロングポーリング
クライアントが送ったHTTPリクエストをサーバーが保持し、新しいメッセージが到着した瞬間にレスポンスを返します。応答後はクライアントがすぐ次のリクエストを張り直します。
無駄リクエストが削減されるためショートポーリングより効率的です。コネクションさえ生きていれば新規メッセージ届いた際に即座に取得可能です。
欠点
* 結局再接続の際にはTLSハンドシェイクが毎回発生します。
* ロングポーリングはコネクションを長時間維持するため、多数のクライアントが同時接続するとサーバーのリソースを消費し、TCPポートの枯渇やファイルディスクリプタの上限で問題になり、工夫を考えなければなりません。
* HTTPサーバーは基本的にステートレスのため、ラウンドロビンでロードバランシングすると送信者と受信者が別サーバーに接続します。その結果、受信側と長いポーリング接続を保持していないサーバーがメッセージを受け取り、即時配信できない恐れがあります。
￼

WebSocket（オススメ）
初回だけHTTP Upgradeで双方向ソケットを確立し、その後はヘッダレスの軽量フレームを全二重で交換します。サーバーはいつでもクライアントへプッシュ可能です。
* HTTPのリクエスト/レスポンスモデルを超えた、継続的な双方向通信
* 継続的なコネクションを保持するので、ポーリングやSSEと比べてサーバーへの負荷を軽減できる
* 従来のHTTPより少ないオーバーヘッドで通信可能（各フレームに追加されるオーバーヘッドは比較的少なく、2～14バイトでHTTPに比べ遥かに小さい）
欠点
* ステートフルな接続をスケールさせるのは簡単な問題ではありません。どのような手法が考えられるかは次で解説していきます。
￼


2. WebSocketサーバーのスケールアウト
同時接続1億を考える上で課題となるのは大量のコネクション数がサーバーのファイルディスクリプタやメモリの枯渇に直結する点です。
WebSocketはTCPを活用しています。そのため、各接続はOSリソース（メモリ、ファイルディスクリプタ）を専有し、接続数が増えるほどこれらのリソースを圧迫します。詳しくは下の2019年のGopherconのトークの記事やWebサーバにおけるソケット周りの知識を読むのがおすすめです。
￼
Speaker Deck
Going Infinite, handling 1M websockets connections in Go
￼
Carpe Diem
Webサーバにおけるソケット周りの知識 - Carpe Diem
このようなOSリソースの制約上、同時接続1億を満たすには1台のサーバーで処理する事は現実的ではないためスケールアウト（水平スケーリング）を目指すことになります。しかし、WebSocketのスケールアウトは難しい問題です。各サーバーがコネクションを長時間保持し、サーバー間の関連するコネクション同士を管理するのは、従来のHTTPとは異なる設計上の制約が生じます。
関連 - 
📗
ステートフル接続に対してスケーラブルな設計を行う
 

ロードバランサー + Pub/Sub
まずはロードバランサーでWebSocketのコネクションを分散し、Pub/Subを経由して各チャネルに対応するWebSocketコネクションを共有する手法を考えます。これにより、同じチャネルに属するユーザーが異なるWebSocketサーバーに接続していても、Pub/Subを通じてメッセージを届けることができます。


ユーザーIDごとにPub/Subトピックを分割
具体的な実装では、ユーザーIDごとにPub/Subトピックを分割して管理します。各WebSocketサーバーは、自身に接続しているユーザーのトピック（例：user:123456）をサブスクライブし、そのユーザー宛のメッセージを受信できるようにします。
メッセージ送信時の流れは以下のようになります：
1. 1対1チャット：送信者のWebSocketサーバーが受信者のトピック（user:受信者ID）にメッセージをPublish
2. グループチャット：メッセージ送信時に、そのチャネルに参加している全ユーザーをMembershipsテーブルから取得し、各ユーザーのトピック（user:参加者ID1, user:参加者ID2, ...）に対してメッセージをPublish


Pub/Subブローカーのシャーディング
同時接続1億という規模を考慮すると、Pub/Subもシャード化して管理することが重要です。例えば、ユーザーIDのハッシュ値に基づいて複数のクラスターに分散させることで、単一のPub/Subブローカーへの負荷集中を回避できます。各WebSocketサーバーは、必要なPub/Subブローカーのシャードに対してのみ接続を維持し、トピック管理の効率化を図ります。
￼


課題
同時接続1億を支えるためのシャード数の増加に伴い、各シャードの監視、パフォーマンス調整、障害対応の運用コストが増大します。例えば、LINEの場合同時接続数30万超のサービスで、24シャード、48ノード構成のRedis ClusterでPub/Subのメッセージ配信を行っていました。
￼
LINE ENGINEERING
同時接続数30万超のチャットサービスのメッセージ配信基盤をRedis Pub/SubからRedis Streamsにした話
これを同時接続1億規模まで拡大するとなると、単純にシャードの必要な数を計算すると必要なシャード数は約333倍となり、8,000シャード以上、16,000ノード以上の大規模Redis Cluster構成が必要になる計算です。
もちろんこのような単純な話ではありませんが、これだけの規模のインフラストラクチャを管理するための運用チームの拡大、監視システムの複雑化、コスト増大も避けられません。そのため、同時接続1億という規模に対してスケーラビリティとコスト効率を考慮に入れなければなりません。

Consistent Hashing + コーディネーションサービス（オススメ）
より効率的で大規模な構成を実現するため、コーディネーションサービスとConsistent Hashingを組み合わせたアーキテクチャを考えます。


コーディネーションサービスを利用したサービスディスカバリ
コーディネーションサービス（ZooKeeper、Consul、etcd等）を使用して、WebSocketサーバーの登録・管理を行います。各WebSocketサーバーは起動時にコーディネーションサービスに自身の状態を登録し、ヘルスチェックを通じて生存状況を報告します。
Consistent Hash Ring Managerがこの情報を基にハッシュリングを構築・管理し、ユーザーIDのハッシュ値に応じて適切なWebSocketサーバーを決定します。この仕組みにより、クライアントは常に一貫したサーバーに接続でき、サーバーの追加・削除時も最小限の再配置で済みます。
Service Discoveryコンポーネントは、クライアントからの接続要求に対して「どのWSに接続すべきか」をConsistent Hashingの結果に基づいて応答し、効率的な負荷分散を実現します。
￼


どのように異なるWebSocketサーバー間でメッセージを送信するか
クライアント間のメッセージ送信は以下の流れで行われます
1. 送信者からの受信: Client1がWS1にメッセージを送信します
2. Chat Serviceへの転送: WS1が受信したメッセージをChat Serviceに転送し、メッセージの永続化とルーティング処理を依頼します
3. メッセージの永続化: Chat ServiceがメッセージをChat KV Storeに保存し、配信履歴と整合性を管理します
4. 宛先サーバーの特定: またChat KV Storeから転送すべきClientの情報（Client4）を取得、Chat ServiceがCoordination Serviceに問い合わせ、Consistent Hash Ringの状態からClient4がWS2に接続されていることを判断します
5. サーバー間メッセージ転送: Chat ServiceからWS2に直接メッセージを転送します
6. クライアントへの配信: WS2が接続中のClient4にメッセージを配信します
￼
 
この方式では、Pub/Subブローカーを経由せずにWebSocketサーバー間をChat Serviceが直接通信を行うため、レイテンシーが低く、シャーディングも必要がなくメッセージ配信に関してはシンプルなアーキテクチャを実現できます。


課題
Consistent Hash Ringを使用したWebSocketサーバー構成において、サーバーのスケールイン・アウト時に発生する最大の課題は、既存のクライアント-サーバーマッピングの一部破綻です。
新サーバー追加時に既存サーバーの担当範囲が縮小されると、範囲外となったクライアントは物理的には旧サーバーに接続したままの状態になります。この結果、他クライアントが新しいConsistent Hash Ringを参照してメッセージを送信すると、実際の接続先とは異なるサーバーに送られてしまい、メッセージが届かない問題が発生します。
この問題を解決するため、マイグレーション期間中は該当するクライアント宛のメッセージを旧サーバーと新サーバーの両方に重複送信することで、接続移行完了まで確実な配信を保証する必要があります。
詳しくは次の「コーディネーションサービス + Consistent Hashingの課題」を参照してください。
📗
ステートフル接続に対してスケーラブルな設計を行う
 


「ロードバランサー + Pub/Sub」と「Consistent Hashing + コーディネーションサービス」の比較
どちらの手法も現実的に要件を満たす事はできますが、Consistent Hashing + コーディネーションサービスの構成の複雑性やサーバーのスケーリングによる接続のマイグレーションロジックの複雑性という課題はあります。
しかし、Pub/Subブローカーがスケーラビリティのボトルネックになる可能性が高いこと、耐障害性を考慮してもメッセージ送信に対して一つコンポーネント（Pub/Subブローカー）を減らせるという点において利点があります。
Pub/Sub方式を選んだとしても一概に間違いと言えるものはなく、どちらかの手法に拘るよりは何故その手法を選んだのかをトレードオフを説明できることが大切です。


3. オンラインプレゼンスの実装


ハートビートメッセージでプレセンスを管理
オンラインプレセンスを実装する上で、WebSocketの接続の有無で判断する手法もあるが、これは基本的に適切ではないです。我々はモバイルネットワークであったり不安定なネットワークにいる中で接続が切れてオフラインになったり、再接続が完了したらオンラインになる事は頻繁にオンラインプレゼンスのステータスが変更されることなるため採用はしません。
その代わりハートビートメッセージを用います。クライアントは自身がアクティブな場合一定（例: 10秒）の間隔でHeartbeatを送ってステータス更新します。追加の想定質問から得た情報により30秒ハートビートがなければオフライン扱いとなります。
￼


ハートビートメッセージの設計
1. 初期接続時には一括オンラインプレセンスを取得: まずユーザーは初期ではクライアントがWebSocketサーバーに接続した時点で、Presence Serviceに対して各ユーザーのUserPresenceレコードを取得するリクエストを送信します。
2. 接続中はリアルタイムに情報を取得: 次にオンラインプレセンスの情報をどのように伝達するかですが、Consistent Hashing + コーディネーションサービスを利用している場合、通常のメッセージと同じ形式でそれぞれが接続しているWebSocketに転送します。これでユーザーがオンラインになった際リアルタイムで接続しているクライアントに状態を転送できます。
￼


オフラインを実現する遅延トリガーについて
オンラインと違ってオフラインの判定は単純ではありません。それは長い間ハートビートが送られていない事をシステムで検知することが困難だからです。
実現する方法の一つとして各WebSocketサーバーはstatus=onlineのメッセージを受け取ったら遅延トリガーをセットします。具体的には、ハートビート受信時に該当ユーザーの遅延トリガーを設定または更新し、一定時間（例：30秒）後にタイムアウト処理を実行するタイマーを管理します。
各ユーザーのハートビートを受信する度に、既存の遅延トリガーをキャンセルして新しいタイマーを設定することで、連続的にハートビートが送信されている間はオンライン状態を維持し、ハートビートが途絶えた場合のみオフライン処理が実行されます。この方式により、短時間のネットワーク切断では状態変更が発生せず、安定したプレゼンス管理が実現できます。
また、Consistent Hashingで接続しているクライアントのサーバーが一意に決まるため、ハートビートは常に同じWebSocketサーバーで受け取れるので重複して複数サーバーで遅延トリガーが設定される事はありません。

class DelayedTriggerManager:
    def handle_heartbeat(self, user_id):
        # 既存のトリガーをキャンセル
        if user_id in self.triggers:
            self.triggers[user_id].cancel()
        
        # 30秒後にオフライン処理を実行するタイマーを設定
        self.triggers[user_id] = Timer(30, self.execute_offline, user_id)
    
    def execute_offline(self, user_id):
        self.send_offline_event_to_presence_service(user_id)
        del self.triggers[user_id]


WebSocketサーバークラッシュ時の遅延トリガー消失への対策
WebSocketサーバーがクラッシュすると、プロセス内で保持していた遅延トリガーはすべて失われます。この問題に対してはどのように対処すべきでしょうか？

遅延キューを使った処理
最も確実な方法として、ハートビート受信時に遅延キューにオフライン処理のジョブを追加し、次のハートビート受信時に前回のジョブをキャンセルする方式があります。これによりWebSocketサーバーがクラッシュしても、キューに残ったジョブが確実にオフライン処理を実行できます。例えば、Amazon SQS遅延キューの機能やRedis Sorted Setsを使って遅延キューは実装できます。
￼
LINE ENGINEERING
RedisのSorted Setsで簡易的な遅延実行Queueを作って迅速にLINE LIVEのPC配信対応をリリースした話
￼
Amazon Simple Queue Service
Amazon SQS 遅延キュー - Amazon Simple Queue Service
利点
関連
* 📗 遅延キューを使って処理を遅らせる  


課題
しかし、同時接続1億人のユーザーがいる場合を考えてみましょう。各ユーザーが10秒間隔でハートビートを送信すると、毎秒1000万件のキューの追加・削除操作が発生します。これを処理する遅延キューシステムを構築するには、複数のシャードに分けて対応する必要があり、システム全体が複雑化してしまいます。
さらに重要な点として、コーディネーションサービス + Consistent Hashingによって、特定のクライアントは常に同じサーバーへアクセスできる仕組みが既に構築されています。この環境では、基本的に共有ストレージのような外部依存を極力導入しなくても要件を満たすことができます。
このため、全ユーザーのハートビートを遅延キューで管理するのは、発生頻度の低い異常事態（WebSocketサーバーのクラッシュ）に対して行うもので、コストパフォーマンスの観点で導入には慎重になるべきです。

クリーナップ処理によるフォールバック処理（オススメ）
次に、WebSocketサーバーがクラッシュした場合にオンラインプレセンスの別サービスでクリーンアップ処理によるフォールバック機能を考えます。
どのような手法かというと、WebSocketサーバーがクラッシュしてConsistent Hash Ringから外された際に、一定時間後にそのサーバーが担当していたユーザー範囲のPresenceデータベースをスキャンし、status=onlineかつlastEventAtが一定時間（例：10分）以上経過したユーザーを検出してオフライン処理を実行します。
status=onlineでlastEventAtが10分以上古いデータを持っているケースは、実際にはサーバークラッシュなどの異常事態でのみ発生します。通常のユーザーはWebSocketサーバークラッシュした場合、クライアントは数秒から数十秒以内に他のサーバーに接続するケースが殆どです。再びハートビートメッセージが送信されるので、status=onlineでlastEventAt が極端に古くなる例は稀というわけです。
なので、タイミングによっては検索条件に該当するユーザーが全て取得されてしまいますが、大抵は少量のレコードの処理だけで済みます。


4. グローバルスケールで低レイテンシを実現するには？
非機能要件に含まれる世界中どこからでも250ms以下のレイテンシを実現するには、各地域にWebSocketサーバーを分散配置する必要があります。ユーザーからもっとも近い地域のサーバーに接続することで、物理的な距離による遅延を最小化できます。
各リージョンには完全なWebSocketゲートウェイサーバー群を配置し、ユーザーは地理的に最も近いリージョンのゲートウェイに接続します。例えば、東京、シンガポール、ロンドン、バージニア、オレゴンなどの主要リージョンにサーバーを配置することで、世界中のユーザーからの接続レイテンシを大幅に短縮できます。
リージョン間でのメッセージ配信が必要な場合は、専用ネットワークを経由してリージョン間通信を行います。これにより、パブリックインターネットの利用区間をできるだけ短くして、長い区間に対して高品質の通信経路を確保することでグローバルユーザー間のチャットでも低レイテンシを維持できます。
￼
実際の例として、Slackでは各リージョンごとにWebSocketで接続するGateway Servers（GS）を用意し、ユーザーはもっとも近いGSに接続するようにしています。
￼
Engineering at Slack
Real-time Messaging - Engineering at Slack
関連
* 📗 ネットワークレイテンシを改善する  


5. メッセージデータベースの可用性や信頼性について
非機能要件から高可用かつ高信頼性なサービス提供を要求されています。特にデータベースはサービスの中心に位置します。仮にデータベースに障害が発生してサービスが停止した場合、同時接続が最大1億人いると考えるとメッセージの送受信ができなくなるだけで相当なビジネス的損失が考えられます。また、障害によりメッセージデータが消失することは防ぎたい自体です。

シングルリーダーレプリケーション
シングルリーダーレプリケーションでは、1つのプライマリDB（リーダー）がすべての書き込みを受け付け、複数のレプリカDB（フォロワー）が読み取り専用として動作します。プライマリDBは書き込み処理後に変更内容をレプリカDBに非同期で転送し、すべてのノードでデータの一貫性を維持します。
プライマリDB障害時には自動フェイルオーバー機構により、レプリカDBの1つを新しいプライマリに昇格させることで、サービスの継続性を確保できます。例えば、AmazonのRDSマルチAZ構成では、自動フェイルオーバーが60〜120秒以内で完了します。
レプリケーションにより、万が一障害が発生した場合でもデータはレプリケーションにより複製されているため、安全にメッセージデータを復旧できます。
関連
* 📗 レプリケーションによる耐障害性、可用性の向上   


課題
シングルリーダーレプリケーションでは可用性に関しては、プライマリDBに障害が発生して、スタンバイDBにフェイルオーバープロセスが完了するまでの間、すべての書き込み操作が停止してしまいます。この間ユーザーはメッセージを送信できません。今回のケースでは一貫性よりもっと高い可用性を重視する必要がありそうです。

リーダーレスレプリケーション（オススメ）
リーダーレスレプリケーションでは、すべてのノードが対等に読み書きを受け付ける分散アーキテクチャを採用します。クライアントは複数のノードに対して並行して書き込みリクエストを送信し、設定された数のノードから成功応答を受け取った時点で書き込み完了と判定します。
高可用性の実現が最大の利点です。クオラム（Quorum）により、任意のノードが故障しても、残りのノードで継続的にサービスを提供できるため、単一障害点が存在しません。また、フェイルオーバー処理が不要なため、ノード障害時もサービス停止なしで継続運用が可能です。例えば、5ノード構成でレプリケーション係数を3に設定した場合、2ノードまでの同時障害に耐えられます。
実際に、Discordではリーダーレスレプリケーションを使っているCassandra→ScyllaDBを使って何兆ものメッセージを保存しています。
￼
How Discord Stores Billions of Messages
￼
How Discord Stores Trillions of Messages
関連 
* 📗 レプリケーションによる耐障害性、可用性の向上  


課題
一方で、結果整合性モデルを採用するため、短時間のデータ不整合が発生する可能性があります。ノード間での競合解決やクオラム（Quorum）制御も複雑になるため、運用面での専門知識が要求されます。


6. メッセージデータベースのスケールアウト
WebSocketサーバーに関するスケールアウトの話はできてますが、データベースに関しても大量の書き込みが予想される中でスケールアウトする戦略を考えなくてはなりません。

MODでのシャーディング
最もシンプルなシャーディング手法として、チャネルIDのハッシュ値をノード数で割った余りによってデータを分散する方法があります。例えば、hash(channelId) % nodeCountの計算結果に基づいて、各メッセージを適切なデータベースノードに格納します。
この方式の利点は実装が非常に簡単で理解しやすく、データの分散も比較的均等になることです。また、特定のチャネルのメッセージは常に同じノードに格納されるため、そのチャネル内でのメッセージ順序を簡単に保証できます。
関連
* 📗 パーティショニングによるスケーラビリティの向上   


課題
一方で、ノードの追加や削除時にデータの大規模な再配置が必要となり、リバランス処理中にサービス停止が発生するリスクがあります。例えば、3ノードから4ノードに拡張する場合、約75％のデータが異なるノードに移動する必要があり、運用負荷が非常に高くなります。

Consistent Hashingをつかったパーティショニング（オススメ）
Consistent Hashingでは、ハッシュ値空間をリング状に配置し、各ノードがリング上の複数の仮想ポイントを担当します。チャネルIDをハッシュした結果から時計回りに最初に見つかるノードがそのデータを格納します。この方式により、ノードの追加や削除時に影響を受けるデータ量を全体の一部に限定できます。
ノードを1台追加した場合、再配置が必要なデータは平均して全体の1/N（Nは総ノード数）程度に抑えられるため、運用中でも比較的安全にスケールアウトが可能です。また、仮想ノードの概念により負荷分散を細かく調整でき、ノード性能の違いにも対応できます。分散キーバリューストアとの相性も良く、多くの実装でConsistent Hashingが標準的なパーティショニング手法として採用されています。
関連
* 📗 パーティショニングによるスケーラビリティの向上   
* 📗 Consistent Hashing（コンシステントハッシュ法）   

画像アップロードサービスの設計
YouTube Liveコメントサービスの設計





















目次
要件
機能要件
非機能要件
想定質問
データモデル
API
チャネルを作成する
メッセージを送信する
メッセージを受信する
チャネルに参加・離脱する
オンラインプレセンス（heartbeat）を送信する
オンラインプレセンス通知を受信する
アーキテクチャ図
設計の深堀りについて
1. リアルタイム通信方式の比較
ショートポーリング
ロングポーリング
WebSocket（オススメ）
2. WebSocketサーバーのスケールアウト
ロードバランサー + Pub/Sub
Consistent Hashing + コーディネーションサービス（オススメ）
「ロードバランサー + Pub/Sub」と「Consistent Hashing + コーディネーションサービス」の比較
3. オンラインプレゼンスの実装
ハートビートメッセージでプレセンスを管理
ハートビートメッセージの設計
オフラインを実現する遅延トリガーについて
WebSocketサーバークラッシュ時の遅延トリガー消失への対策
遅延キューを使った処理
クリーナップ処理によるフォールバック処理（オススメ）
4. グローバルスケールで低レイテンシを実現するには？
5. メッセージデータベースの可用性や信頼性について
シングルリーダーレプリケーション
リーダーレスレプリケーション（オススメ）
6. メッセージデータベースのスケールアウト
MODでのシャーディング
Consistent Hashingをつかったパーティショニング（オススメ）
© 2026 InterviewCat. All rights reserved.
プライバシーポリシー
利用規約
特定商取引法に基づく表記
運営
お問い合わせフォーム

LINE（リアルタイムチャット）の設計 - SystemDesign InterviewCat | InterviewCat - テック企業面接対策プラットフォーム
